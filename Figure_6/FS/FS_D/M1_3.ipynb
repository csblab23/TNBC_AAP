{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a7c280-24a8-4482-ae05-c4b5fc289668",
   "metadata": {},
   "source": [
    "# script - Feature set - D (12 feature model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea20b8-639d-4210-8b80-87f590fadf1a",
   "metadata": {},
   "source": [
    "### the following section will be run for model construction (LASSO, RF, ELASTIC) using three different feature set i.e. \n",
    "#### All clinical features only (8)\n",
    "#### All promoter features only (4)\n",
    "#### clinical + promoter features (12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e1ef2d-8049-4fa8-b21d-c5080fb97201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c33a563-8e77-4425-b09d-25ce459b350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "random_state = int(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed5a60a-c190-4b03-95c3-f11ffa86d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing librariers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sksurv.util import Surv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.util import Surv\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score, cumulative_dynamic_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e8a238-71ed-475e-917a-6ee90d895eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the all dataset---\n",
    "input_all = pd.read_csv(\"./../../Input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c630403-599e-4e7f-8845-8286d27bd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state=1\n",
      "Training set size: (242, 24)\n",
      "Validation set size: (105, 24)\n",
      "\n",
      "Distribution of mRNA_Subtype in training set:\n",
      "mRNA_Subtype\n",
      "BLIS    0.396694\n",
      "IM      0.243802\n",
      "LAR     0.214876\n",
      "MES     0.144628\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of mRNA_Subtype in validation set:\n",
      "mRNA_Subtype\n",
      "BLIS    0.400000\n",
      "IM      0.238095\n",
      "LAR     0.209524\n",
      "MES     0.152381\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 70% training and 30% validation while stratifying by 'mRNA_Subtype'\n",
    "print(f\"random_state={random_state}\")\n",
    "train_df, val_df = train_test_split(input_all, test_size=0.30, stratify=input_all['mRNA_Subtype'], random_state=random_state)\n",
    "\n",
    "# Print the sizes of the splits to verify\n",
    "print(\"Training set size:\", train_df.shape)\n",
    "print(\"Validation set size:\", val_df.shape)\n",
    "\n",
    "# Check the distribution of 'mRNA_Subtype' in both splits\n",
    "print(\"\\nDistribution of mRNA_Subtype in training set:\")\n",
    "print(train_df['mRNA_Subtype'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribution of mRNA_Subtype in validation set:\")\n",
    "print(val_df['mRNA_Subtype'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ebb8adf-0887-42ad-98cf-3cfbca983060",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"clinical_only\":['Ki67', 'Size_cm', 'Age_at_surgery', 'CNA_Subtype_Chr12p13_amp', 'CNA_Subtype_Chr8p21_del',\n",
    "              'CNA_Subtype_Chr9p23_amp', 'CNA_Subtype_Chr13q34_amp', 'CNA_Subtype_Chr20q13_amp'],\n",
    "    \"promoter_only\": ['pr3004_huwe1', 'pr9001_cald1', 'pr12191_tank', 'pr19936_tpm4' ],\n",
    "    \"clinical_plus_promoter\": ['Ki67', 'Size_cm', 'Age_at_surgery', 'CNA_Subtype_Chr12p13_amp', 'CNA_Subtype_Chr8p21_del',\n",
    "              'CNA_Subtype_Chr9p23_amp', 'CNA_Subtype_Chr13q34_amp', 'CNA_Subtype_Chr20q13_amp',\n",
    "                              'pr3004_huwe1', 'pr9001_cald1', 'pr12191_tank', 'pr19936_tpm4']\n",
    "}\n",
    "# Define target\n",
    "y = train_df[['RFS_Status', 'RFS_time_Months']]\n",
    "#now convert the y(target matrix) in a structured array:\n",
    "y_surv = Surv.from_dataframe('RFS_Status', 'RFS_time_Months', y) #false: 0, true: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fd7576-2a26-42af-b583-3219dd97d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running for: clinical_only\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Hyperparameters: {'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "10\n",
      "2\n",
      "500\n",
      "Best L1 Ratio: 0.4\n",
      "alpha_lasso =  [0.0008823078982141463]\n",
      "alpha_elastic =  [0.0012048525828495232]\n",
      "\n",
      "Running for: promoter_only\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Hyperparameters: {'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "10\n",
      "2\n",
      "1000\n",
      "Best L1 Ratio: 0.8\n",
      "alpha_lasso =  [0.015240128403696722]\n",
      "alpha_elastic =  [0.0190501605046209]\n",
      "\n",
      "Running for: clinical_plus_promoter\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Hyperparameters: {'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "5\n",
      "2\n",
      "500\n",
      "Best L1 Ratio: 0.2\n",
      "alpha_lasso =  [0.0023434872292439606]\n",
      "alpha_elastic =  [0.0017400020792129722]\n"
     ]
    }
   ],
   "source": [
    "# Loop over feature sets\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"\\nRunning for: {name}\")\n",
    "    \n",
    "    # Select features\n",
    "    X = train_df[features]\n",
    "    #defining the different models:\n",
    "\n",
    "    #1) LASSO ------------------hyperparameter -- alpha\n",
    "    coxnet_pipe_lasso = make_pipeline( CoxnetSurvivalAnalysis(l1_ratio = 1.0, alpha_min_ratio = 0.01, max_iter=100))\n",
    "    #3) ELASTICNET -------------hyperparameters -- L1 and alpha\n",
    "    coxnet_pipe_elastic = make_pipeline(CoxnetSurvivalAnalysis(max_iter=100, alpha_min_ratio= 0.01))\n",
    "\n",
    "    #defining random survival forest plot -------------hyperparameters ----n_estimators, min_samples_split, min_samples_leaf\n",
    "    ##search for best hyperparameters for random survival forest\n",
    "    \n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 500,1000],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [5, 10, 15]\n",
    "    }\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    \n",
    "    grid_search_rf = GridSearchCV(\n",
    "        estimator=RandomSurvivalForest(random_state=random_state, n_jobs=1),\n",
    "        param_grid=param_grid_rf,\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        n_jobs=1,\n",
    "        error_score=0.5,\n",
    "        verbose=1\n",
    "    ).fit(X, y_surv)\n",
    "    print(\"Best Hyperparameters:\", grid_search_rf.best_params_)\n",
    "    best_model = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Extract best hyperparameters\n",
    "    min_samples_leaf = grid_search_rf.best_params_['min_samples_leaf']\n",
    "    min_samples_split = grid_search_rf.best_params_['min_samples_split']\n",
    "    n_estimators = grid_search_rf.best_params_['n_estimators']\n",
    "    print(min_samples_leaf)\n",
    "    print(min_samples_split)\n",
    "    print(n_estimators)\n",
    "\n",
    "    #search for best l1 for elastic net\n",
    "    # Set up the parameter grid\n",
    "    param_grid_l1_elastic = {'coxnetsurvivalanalysis__l1_ratio': [0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    GSV_elastic_L1 = GridSearchCV(\n",
    "        coxnet_pipe_elastic,\n",
    "        param_grid_l1_elastic,\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X, y_surv)\n",
    "    \n",
    "    \n",
    "    # Best parameters\n",
    "    best_l1_ratio = GSV_elastic_L1.best_params_['coxnetsurvivalanalysis__l1_ratio']\n",
    "    print(f\"Best L1 Ratio: {best_l1_ratio}\")\n",
    "    #fit elastic model with best l1 ratio:\n",
    "    coxnet_pipe_elastic = make_pipeline( CoxnetSurvivalAnalysis(l1_ratio=best_l1_ratio, alpha_min_ratio = 0.01, max_iter=100))  ### range (0.0, 1.0] notice the round bracket\n",
    "\n",
    "    coxnet_pipe_lasso.fit(X, y_surv)\n",
    "    coxnet_pipe_elastic.fit(X, y_surv)\n",
    "    \n",
    "    #now i have to search for best alpha for each model\n",
    "    estimated_alphas_lasso = coxnet_pipe_lasso.named_steps['coxnetsurvivalanalysis'].alphas_\n",
    "    estimated_alphas_elastic = coxnet_pipe_elastic.named_steps['coxnetsurvivalanalysis'].alphas_\n",
    "\n",
    "    #now the training dataset is divided into 3-folds (stratified based on number of events in each class), and for that perform grid search\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    \n",
    "    #cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    GSV_alpha_lasso = GridSearchCV(\n",
    "        coxnet_pipe_lasso,\n",
    "        param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in estimated_alphas_lasso]},\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X, y_surv)\n",
    "    \n",
    "    GSV_alpha_elastic = GridSearchCV(\n",
    "        coxnet_pipe_elastic,\n",
    "        param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in estimated_alphas_elastic]},\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X, y_surv)\n",
    "\n",
    "    alpha_lasso = GSV_alpha_lasso.best_params_[\"coxnetsurvivalanalysis__alphas\"]\n",
    "    alpha_elastic = GSV_alpha_elastic.best_params_[\"coxnetsurvivalanalysis__alphas\"]\n",
    "    \n",
    "    \n",
    "    print(\"alpha_lasso = \",alpha_lasso)\n",
    "    print(\"alpha_elastic = \",alpha_elastic)\n",
    "\n",
    "    #now using the above calculated alpha for model construction: and evaluating the performance on training dataset itself\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    c_indices_lasso = []\n",
    "    c_indices_elastic_net = []\n",
    "    c_indices_rf = []\n",
    "    \n",
    "    ibs_scores_lasso = []\n",
    "    ibs_scores_elastic_net = []\n",
    "    ibs_scores_rf = []\n",
    "    \n",
    "    fold_numbers = []\n",
    "    \n",
    "    time_dependent_aucs_lasso = []\n",
    "    time_dependent_aucs_elastic_net = []\n",
    "    time_dependent_aucs_rf = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X, event_occurrences), 1):\n",
    "        x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_surv[train_index], y_surv[test_index]\n",
    "    \n",
    "        #defining four models\n",
    "        model_lasso = CoxnetSurvivalAnalysis(alphas=alpha_lasso, fit_baseline_model=True, l1_ratio=1.0).fit(x_train, y_train)\n",
    "        model_elastic = CoxnetSurvivalAnalysis(alphas=alpha_elastic, fit_baseline_model=True, l1_ratio=best_l1_ratio).fit(x_train, y_train)\n",
    "        model_rf = RandomSurvivalForest(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=1, random_state=random_state).fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "        #c-index calculation for each model\n",
    "        c_index_lasso = concordance_index_censored(y_test['RFS_Status'], y_test['RFS_time_Months'], model_lasso.predict(x_test))[0]\n",
    "        c_indices_lasso.append(c_index_lasso)\n",
    "    \n",
    "        c_index_elastic = concordance_index_censored(y_test['RFS_Status'], y_test['RFS_time_Months'], model_elastic.predict(x_test))[0]\n",
    "        c_indices_elastic_net.append(c_index_elastic)\n",
    "    \n",
    "        c_index_rf = concordance_index_censored(y_test['RFS_Status'], y_test['RFS_time_Months'], model_rf.predict(x_test))[0]\n",
    "        c_indices_rf.append(c_index_rf)\n",
    "    \n",
    "        #ibs calculation for each model\n",
    "        times = np.percentile(y_test['RFS_time_Months'], np.linspace(5, 95, 20))\n",
    "    \n",
    "        surv_funcs_lasso = model_lasso.predict_survival_function(x_test)\n",
    "        surv_probs_lasso = np.asarray([[fn(t) for t in times] for fn in surv_funcs_lasso])\n",
    "        ibs_lasso = integrated_brier_score(y_train, y_test, surv_probs_lasso, times)\n",
    "        ibs_scores_lasso.append(ibs_lasso)\n",
    "    \n",
    "        surv_funcs_elastic = model_elastic.predict_survival_function(x_test)\n",
    "        surv_probs_elastic = np.asarray([[fn(t) for t in times] for fn in surv_funcs_elastic])\n",
    "        ibs_elastic = integrated_brier_score(y_train, y_test, surv_probs_elastic, times)\n",
    "        ibs_scores_elastic_net.append(ibs_elastic)\n",
    "    \n",
    "        surv_funcs_rf = model_rf.predict_survival_function(x_test)\n",
    "        surv_probs_rf = np.asarray([[fn(t) for t in times] for fn in surv_funcs_rf])\n",
    "        ibs_rf = integrated_brier_score(y_train, y_test, surv_probs_rf, times)\n",
    "        ibs_scores_rf.append(ibs_rf)\n",
    "    \n",
    "        #Time-dependent ROC AUC calculation for each model\n",
    "        auc_lasso, mean_auc_lasso = cumulative_dynamic_auc(y_train, y_test, model_lasso.predict(x_test), times)\n",
    "        time_dependent_aucs_lasso.append(mean_auc_lasso)\n",
    "    \n",
    "        auc_elastic, mean_auc_elastic = cumulative_dynamic_auc(y_train, y_test, model_elastic.predict(x_test), times)\n",
    "        time_dependent_aucs_elastic_net.append(mean_auc_elastic)\n",
    "    \n",
    "        auc_rf, mean_auc_rf = cumulative_dynamic_auc(y_train, y_test, model_rf.predict(x_test), times)\n",
    "        time_dependent_aucs_rf.append(mean_auc_rf)\n",
    "    \n",
    "        #adding fold number info\n",
    "        fold_numbers.append(fold)\n",
    "\n",
    "\n",
    "    #calculating mean measures:\n",
    "    mean_c_index_lasso = np.mean(c_indices_lasso)\n",
    "    mean_c_index_elastic = np.mean(c_indices_elastic_net)\n",
    "    mean_c_index_rf = np.mean(c_indices_rf)\n",
    "    \n",
    "    mean_ibs_lasso = np.mean(ibs_scores_lasso)\n",
    "    mean_ibs_elastic = np.mean(ibs_scores_elastic_net)\n",
    "    mean_ibs_rf = np.mean(ibs_scores_rf)\n",
    "    \n",
    "    mean_auc_lasso = np.mean(time_dependent_aucs_lasso)\n",
    "    mean_auc_elastic = np.mean(time_dependent_aucs_elastic_net)\n",
    "    mean_auc_rf = np.mean(time_dependent_aucs_rf)\n",
    "    \n",
    "    #calculating standard deviation of measures:\n",
    "    std_c_index_lasso = np.std(c_indices_lasso)\n",
    "    std_c_index_elastic = np.std(c_indices_elastic_net)\n",
    "    std_c_index_rf = np.std(c_indices_rf)\n",
    "    \n",
    "    std_ibs_lasso = np.std(ibs_scores_lasso)\n",
    "    std_ibs_elastic = np.std(ibs_scores_elastic_net)\n",
    "    std_ibs_rf = np.std(ibs_scores_rf)\n",
    "    \n",
    "    std_auc_lasso = np.std(time_dependent_aucs_lasso)\n",
    "    std_auc_elastic = np.std(time_dependent_aucs_elastic_net)\n",
    "    std_auc_rf = np.std(time_dependent_aucs_rf)\n",
    "\n",
    "    #now constructing the model, taking all the samples\n",
    "    model_lasso = CoxnetSurvivalAnalysis(alphas=alpha_lasso, fit_baseline_model=True, l1_ratio=1.0).fit(X, y_surv)\n",
    "    model_elastic = CoxnetSurvivalAnalysis(alphas=alpha_elastic, fit_baseline_model=True, l1_ratio=best_l1_ratio).fit(X, y_surv)\n",
    "    model_rf = RandomSurvivalForest(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=1, random_state=random_state).fit(X, y_surv)\n",
    "    #now checking the performance on validation dataset\n",
    "    X_val = val_df[features]\n",
    "    y_val = val_df[['RFS_Status', 'RFS_time_Months']]\n",
    "    y_val_surv = Surv.from_dataframe('RFS_Status', 'RFS_time_Months', y_val) #false: 0, true: 1)\n",
    "\n",
    "\n",
    "    #now calculating matrices for validation dataset ---- by each method\n",
    "\n",
    "    c_index_lasso_test = concordance_index_censored(y_val_surv['RFS_Status'], y_val_surv['RFS_time_Months'], model_lasso.predict(X_val))[0]\n",
    "    c_index_elastic_test = concordance_index_censored(y_val_surv['RFS_Status'], y_val_surv['RFS_time_Months'], model_elastic.predict(X_val))[0]\n",
    "    c_index_rf_test = concordance_index_censored(y_val_surv['RFS_Status'], y_val_surv['RFS_time_Months'], model_rf.predict(X_val))[0]\n",
    "\n",
    "    #simialrly, calculating the ibs score for the validation data ----- for each method\n",
    "    times = np.percentile(y_surv['RFS_time_Months'], np.linspace(5, 95, 20))\n",
    "    \n",
    "    surv_funcs_lasso = model_lasso.predict_survival_function(X_val)\n",
    "    surv_probs_lasso = np.asarray([[fn(t) for t in times] for fn in surv_funcs_lasso])\n",
    "    ibs_test_lasso = integrated_brier_score(y_surv, y_val_surv, surv_probs_lasso, times)\n",
    "    \n",
    "    surv_funcs_elastic = model_elastic.predict_survival_function(X_val)\n",
    "    surv_probs_elastic = np.asarray([[fn(t) for t in times] for fn in surv_funcs_elastic])\n",
    "    ibs_test_elastic = integrated_brier_score(y_surv, y_val_surv, surv_probs_elastic, times)\n",
    "    \n",
    "    surv_funcs_rf = model_rf.predict_survival_function(X_val)\n",
    "    surv_probs_rf = np.asarray([[fn(t) for t in times] for fn in surv_funcs_rf])\n",
    "    ibs_test_rf = integrated_brier_score(y_surv, y_val_surv, surv_probs_rf, times)\n",
    "    \n",
    "    #calculating Time-dependent ROC AUC calculation --- for each method\n",
    "    times = np.percentile(y_surv['RFS_time_Months'], np.linspace(5, 95, 20))\n",
    "    \n",
    "    auc_lasso, mean_auc_lasso_test = cumulative_dynamic_auc(y_surv, y_val_surv, model_lasso.predict(X_val), times)\n",
    "    auc_test_lasso = mean_auc_lasso_test\n",
    "    \n",
    "    auc_elastic, mean_auc_elastic_test = cumulative_dynamic_auc(y_surv, y_val_surv, model_elastic.predict(X_val), times)\n",
    "    auc_test_elastic = mean_auc_elastic_test\n",
    "    \n",
    "    auc_rf, mean_auc_rf_test = cumulative_dynamic_auc(y_surv, y_val_surv, model_rf.predict(X_val), times)\n",
    "    auc_test_rf = mean_auc_rf_test\n",
    "\n",
    "    # Define the file path to save the results\n",
    "    output_file = f'./Output/ME1_3/{name}.csv' #<---------can be changed based on the file name you want\n",
    "    \n",
    "    # Step 1: Check if the DataFrame already exists\n",
    "    if os.path.exists(output_file):\n",
    "        # Load the existing DataFrame from CSV\n",
    "        df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        # If not, create a new empty DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # Step 2: Organize the current run's data into a dictionary (new row)\n",
    "    data = {\n",
    "        'random_state': random_state,\n",
    "        'best_l1_ratio': best_l1_ratio,\n",
    "        'alpha_lasso': alpha_lasso,\n",
    "        'alpha_elastic': alpha_elastic,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'n_estimators': n_estimators,\n",
    "    \n",
    "    \n",
    "        'c_indices_lasso': [c_indices_lasso],\n",
    "        'mean_c_index_lasso': mean_c_index_lasso,\n",
    "        'std_c_index_lasso': std_c_index_lasso,\n",
    "        'c_index_lasso_test': c_index_lasso_test,\n",
    "    \n",
    "        'c_indices_elastic_net': [c_indices_elastic_net],\n",
    "        'mean_c_index_elastic': mean_c_index_elastic,\n",
    "        'std_c_index_elastic': std_c_index_elastic,\n",
    "        'c_index_elastic_test': c_index_elastic_test,\n",
    "    \n",
    "        'c_indices_rf': [c_indices_rf],\n",
    "        'mean_c_index_rf': mean_c_index_rf,\n",
    "        'std_c_index_rf': std_c_index_rf,\n",
    "        'c_index_rf_test': c_index_rf_test,\n",
    "    \n",
    "        'ibs_scores_lasso': [ibs_scores_lasso],\n",
    "        'mean_ibs_lasso': mean_ibs_lasso,\n",
    "        'std_ibs_lasso': std_ibs_lasso,\n",
    "        'ibs_test_lasso': ibs_test_lasso,\n",
    "\n",
    "    \n",
    "        'ibs_scores_elastic_net': [ibs_scores_elastic_net],\n",
    "        'mean_ibs_elastic': mean_ibs_elastic,\n",
    "        'std_ibs_elastic': std_ibs_elastic,\n",
    "        'ibs_test_elastic': ibs_test_elastic,\n",
    "    \n",
    "        'ibs_scores_rf': [ibs_scores_rf],\n",
    "        'mean_ibs_rf': mean_ibs_rf,\n",
    "        'std_ibs_rf': std_ibs_rf,\n",
    "        'ibs_test_rf': ibs_test_rf,\n",
    "    \n",
    "        'time_dependent_aucs_lasso': [time_dependent_aucs_lasso],\n",
    "        'mean_auc_lasso': mean_auc_lasso,\n",
    "        'std_auc_lasso': std_auc_lasso,\n",
    "        'auc_test_lasso': auc_test_lasso,\n",
    "    \n",
    "        'time_dependent_aucs_elastic_net': [time_dependent_aucs_elastic_net],\n",
    "        'mean_auc_elastic': mean_auc_elastic,\n",
    "        'std_auc_elastic': std_auc_elastic,\n",
    "        'auc_test_elastic': auc_test_elastic,\n",
    "    \n",
    "        'time_dependent_aucs_rf': [time_dependent_aucs_rf],\n",
    "        'mean_auc_rf': mean_auc_rf,\n",
    "        'std_auc_rf': std_auc_rf,\n",
    "        'auc_test_rf': auc_test_rf,\n",
    "    \n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame\n",
    "    new_row = pd.DataFrame([data])\n",
    "    \n",
    "    # Step 3: Concatenate the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Step 4: Save the updated DataFrame to CSV (or you can use a different format like pickle)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Now the DataFrame is updated and saved, preserving results between runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378c002-2db0-4a30-a964-9098ed811a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d796c29-f23f-4fb5-a2a4-06f2674f3a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
