{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24de6ad-6ff6-477f-b7c6-d3e2c98d9f06",
   "metadata": {},
   "source": [
    "### Script -- for feature set - \"A\" (4 feature) ---> NRI calculation\n",
    "##### (for comapring models) --- performance on training dataset \n",
    "#### comparing model - clinical only v/s promoters only\n",
    "#### comparing model - clinical only v/s all features\n",
    "#### comparing model - promoter only v/s all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8018d2-6e84-479c-9e3e-0376e968482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df006ab-bfba-4caa-a2bf-cbc6ce54f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "random_state = int(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7898f94-33fc-413a-a0b6-f92098cf585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing librariers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.util import Surv\n",
    "from sksurv.ensemble import RandomSurvivalForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa0c29f-93ae-4e76-a3eb-9b08dff8025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the all dataset---\n",
    "input_all = pd.read_csv(\"./../../Input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c245d9fa-4f85-49cc-806a-e53941a2241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(input_all, test_size=0.30, stratify=input_all['mRNA_Subtype'], random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c8690d-df08-4d22-890e-4ef76dea136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Running comparison: clinical_only vs all_features\n",
      "['Ki67', 'Size_cm']\n",
      "['Ki67', 'Size_cm', 'pr3004_huwe1', 'pr41492_ftx']\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Hyperparameters: {'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "5\n",
      "2\n",
      "100\n",
      "Best L1 Ratio: 0.2\n",
      "alpha_lasso =  [0.01310199226384447]\n",
      "alpha_elastic =  [0.06550996131922238]\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Hyperparameters: {'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "10\n",
      "2\n",
      "100\n",
      "Best L1 Ratio: 0.2\n",
      "alpha_lasso =  [0.0028402985050205967]\n",
      "alpha_elastic =  [0.002209292312950968]\n"
     ]
    }
   ],
   "source": [
    "#now defining feature set for model -01 & model -2\n",
    "feature_sets = {\n",
    "    \"clinical_only\": ['Ki67', 'Size_cm'],\n",
    "    \"promoter_only\": ['pr3004_huwe1', 'pr41492_ftx'],\n",
    "    \"all_features\": ['Ki67', 'Size_cm',  'pr3004_huwe1', 'pr41492_ftx']\n",
    "}\n",
    "\n",
    "model_comparisons = [\n",
    "    (\"clinical_only\", \"all_features\")\n",
    "]\n",
    "\n",
    "for model1_name, model2_name in model_comparisons:\n",
    "    print(f\"\\nüîÅ Running comparison: {model1_name} vs {model2_name}\")\n",
    "    \n",
    "    # Get feature sets\n",
    "    features_1 = feature_sets[model1_name]\n",
    "    features_2 = feature_sets[model2_name]\n",
    "    print(features_1)\n",
    "    print(features_2)\n",
    "\n",
    "    X = train_df[features_1]\n",
    "    y = train_df[['RFS_Status', 'RFS_time_Months']]\n",
    "    y_surv = Surv.from_dataframe('RFS_Status', 'RFS_time_Months', y) #false: 0, true: 1)\n",
    "\n",
    "    #1) LASSO ------------------hyperparameter -- alpha\n",
    "    coxnet_pipe_lasso = make_pipeline( CoxnetSurvivalAnalysis(l1_ratio = 1.0, alpha_min_ratio = 0.01, max_iter=100))\n",
    "    #3) ELASTICNET -------------hyperparameters -- L1 and alpha\n",
    "    coxnet_pipe_elastic = make_pipeline(CoxnetSurvivalAnalysis(max_iter=100, alpha_min_ratio= 0.01))\n",
    "\n",
    "\n",
    "    #defining random survival forest plot -------------hyperparameters ----n_estimators, min_samples_split, min_samples_leaf\n",
    "    ##search for best hyperparameters for random survival forest\n",
    "    \n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 500,1000],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [5, 10, 15]\n",
    "    }\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    \n",
    "    grid_search_rf = GridSearchCV(\n",
    "        estimator=RandomSurvivalForest(random_state=random_state, n_jobs=1),\n",
    "        param_grid=param_grid_rf,\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        n_jobs=1,\n",
    "        error_score=0.5,\n",
    "        verbose=1\n",
    "    ).fit(X, y_surv)\n",
    "    print(\"Best Hyperparameters:\", grid_search_rf.best_params_)\n",
    "    best_model = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Extract best hyperparameters\n",
    "    min_samples_leaf = grid_search_rf.best_params_['min_samples_leaf']\n",
    "    min_samples_split = grid_search_rf.best_params_['min_samples_split']\n",
    "    n_estimators = grid_search_rf.best_params_['n_estimators']\n",
    "    print(min_samples_leaf)\n",
    "    print(min_samples_split)\n",
    "    print(n_estimators)\n",
    "\n",
    "    #search for best l1 for elastic net\n",
    "    # Set up the parameter grid\n",
    "    param_grid_l1_elastic = {'coxnetsurvivalanalysis__l1_ratio': [0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    GSV_elastic_L1 = GridSearchCV(\n",
    "        coxnet_pipe_elastic,\n",
    "        param_grid_l1_elastic,\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X, y_surv)\n",
    "    \n",
    "    \n",
    "    # Best parameters\n",
    "    best_l1_ratio = GSV_elastic_L1.best_params_['coxnetsurvivalanalysis__l1_ratio']\n",
    "    print(f\"Best L1 Ratio: {best_l1_ratio}\")\n",
    "\n",
    "    #fit elastic model with best l1 ratio:\n",
    "    coxnet_pipe_elastic = make_pipeline( CoxnetSurvivalAnalysis(l1_ratio=best_l1_ratio, alpha_min_ratio = 0.01, max_iter=100))  ### range (0.0, 1.0] notice the round bracket\n",
    "    coxnet_pipe_lasso.fit(X, y_surv)\n",
    "    coxnet_pipe_elastic.fit(X, y_surv)\n",
    "\n",
    "    #now i have to search for best alpha for each model\n",
    "    estimated_alphas_lasso = coxnet_pipe_lasso.named_steps['coxnetsurvivalanalysis'].alphas_\n",
    "    estimated_alphas_elastic = coxnet_pipe_elastic.named_steps['coxnetsurvivalanalysis'].alphas_\n",
    "\n",
    "    #now the training dataset is divided into 3-folds (stratified based on number of events in each class), and for that perform grid search\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    \n",
    "    \n",
    "    #cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    GSV_alpha_lasso = GridSearchCV(\n",
    "        coxnet_pipe_lasso,\n",
    "        param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in estimated_alphas_lasso]},\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X, y_surv)\n",
    "    \n",
    "    GSV_alpha_elastic = GridSearchCV(\n",
    "        coxnet_pipe_elastic,\n",
    "        param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in estimated_alphas_elastic]},\n",
    "        cv=cv.split(X, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X, y_surv)\n",
    "\n",
    "\n",
    "    alpha_lasso = GSV_alpha_lasso.best_params_[\"coxnetsurvivalanalysis__alphas\"]\n",
    "    alpha_elastic = GSV_alpha_elastic.best_params_[\"coxnetsurvivalanalysis__alphas\"]\n",
    "    \n",
    "    \n",
    "    print(\"alpha_lasso = \",alpha_lasso)\n",
    "    print(\"alpha_elastic = \",alpha_elastic)\n",
    "\n",
    "    #constructing the model, taking all the samples\n",
    "    model_lasso = CoxnetSurvivalAnalysis(alphas=alpha_lasso, fit_baseline_model=True, l1_ratio=1.0).fit(X, y_surv)\n",
    "    model_elastic = CoxnetSurvivalAnalysis(alphas=alpha_elastic, fit_baseline_model=True, l1_ratio=best_l1_ratio).fit(X, y_surv)\n",
    "    model_rf = RandomSurvivalForest(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=1, random_state=random_state).fit(X, y_surv)\n",
    "\n",
    "    X_val = train_df[features_1]\n",
    "    y_val = train_df[['RFS_Status', 'RFS_time_Months']]\n",
    "    y_val_surv = Surv.from_dataframe('RFS_Status', 'RFS_time_Months', y_val) #false: 0, true: 1)\n",
    "\n",
    "    #for model 2 \n",
    "    X2= train_df[features_2]\n",
    "    y = train_df[['RFS_Status', 'RFS_time_Months']]\n",
    "    y_surv = Surv.from_dataframe('RFS_Status', 'RFS_time_Months', y) #false: 0, true: 1)\n",
    "    #1) LASSO ------------------hyperparameter -- alpha\n",
    "    coxnet_pipe_lasso2 = make_pipeline( CoxnetSurvivalAnalysis(l1_ratio = 1.0, alpha_min_ratio = 0.01, max_iter=100))\n",
    "    #3) ELASTICNET -------------hyperparameters -- L1 and alpha\n",
    "    coxnet_pipe_elastic2 = make_pipeline(CoxnetSurvivalAnalysis(max_iter=100, alpha_min_ratio= 0.01))\n",
    "\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 500,1000],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [5, 10, 15]\n",
    "    }\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    \n",
    "    grid_search_rf = GridSearchCV(\n",
    "        estimator=RandomSurvivalForest(random_state=random_state, n_jobs=1),\n",
    "        param_grid=param_grid_rf,\n",
    "        cv=cv.split(X2, event_occurrences),\n",
    "        n_jobs=1,\n",
    "        error_score=0.5,\n",
    "        verbose=1\n",
    "    ).fit(X2, y_surv)\n",
    "    print(\"Best Hyperparameters:\", grid_search_rf.best_params_)\n",
    "    best_model = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Extract best hyperparameters\n",
    "    min_samples_leaf = grid_search_rf.best_params_['min_samples_leaf']\n",
    "    min_samples_split = grid_search_rf.best_params_['min_samples_split']\n",
    "    n_estimators = grid_search_rf.best_params_['n_estimators']\n",
    "    print(min_samples_leaf)\n",
    "    print(min_samples_split)\n",
    "    print(n_estimators)\n",
    "\n",
    "    #search for best l1 for elastic net\n",
    "    # Set up the parameter grid\n",
    "    param_grid_l1_elastic = {'coxnetsurvivalanalysis__l1_ratio': [0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    GSV_elastic_L1 = GridSearchCV(\n",
    "        coxnet_pipe_elastic2,\n",
    "        param_grid_l1_elastic,\n",
    "        cv=cv.split(X2, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X2, y_surv)\n",
    "    \n",
    "    \n",
    "    # Best parameters\n",
    "    best_l1_ratio = GSV_elastic_L1.best_params_['coxnetsurvivalanalysis__l1_ratio']\n",
    "    print(f\"Best L1 Ratio: {best_l1_ratio}\")\n",
    "\n",
    "    #fit elastic model with best l1 ratio:\n",
    "    coxnet_pipe_elastic2 = make_pipeline( CoxnetSurvivalAnalysis(l1_ratio=best_l1_ratio, alpha_min_ratio = 0.01, max_iter=100))  ### range (0.0, 1.0] notice the round bracket\n",
    "    coxnet_pipe_lasso2.fit(X2, y_surv)\n",
    "    coxnet_pipe_elastic2.fit(X2, y_surv)\n",
    "\n",
    "    #search for best alpha for each model\n",
    "    estimated_alphas_lasso = coxnet_pipe_lasso2.named_steps['coxnetsurvivalanalysis'].alphas_\n",
    "    estimated_alphas_elastic = coxnet_pipe_elastic2.named_steps['coxnetsurvivalanalysis'].alphas_\n",
    "\n",
    "\n",
    "    #training dataset is divided into 3-folds (stratified based on number of events in each class), and for that perform grid search\n",
    "    event_occurrences = y_surv['RFS_Status']\n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sksurv.util import Surv\n",
    "    #cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    GSV_alpha_lasso = GridSearchCV(\n",
    "        coxnet_pipe_lasso2,\n",
    "        param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in estimated_alphas_lasso]},\n",
    "        cv=cv.split(X2, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X2, y_surv)\n",
    "    \n",
    "    GSV_alpha_elastic = GridSearchCV(\n",
    "        coxnet_pipe_elastic2,\n",
    "        param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in estimated_alphas_elastic]},\n",
    "        cv=cv.split(X2, event_occurrences),\n",
    "        error_score=0.5,\n",
    "        n_jobs=1,\n",
    "    ).fit(X2, y_surv)\n",
    "\n",
    "    alpha_lasso = GSV_alpha_lasso.best_params_[\"coxnetsurvivalanalysis__alphas\"]\n",
    "    alpha_elastic = GSV_alpha_elastic.best_params_[\"coxnetsurvivalanalysis__alphas\"]\n",
    "    \n",
    "    \n",
    "    print(\"alpha_lasso = \",alpha_lasso)\n",
    "    print(\"alpha_elastic = \",alpha_elastic)\n",
    "\n",
    "    #constructing the model, taking all the samples\n",
    "    model_lasso_final = CoxnetSurvivalAnalysis(alphas=alpha_lasso, fit_baseline_model=True, l1_ratio=1.0).fit(X2, y_surv)\n",
    "    model_elastic_final = CoxnetSurvivalAnalysis(alphas=alpha_elastic, fit_baseline_model=True, l1_ratio=best_l1_ratio).fit(X2, y_surv)\n",
    "    model_rf_final = RandomSurvivalForest(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=1, random_state=random_state).fit(X2, y_surv)\n",
    "\n",
    "    def check_cat(prob, thresholds):    #the following funstion defines the risk categories \n",
    "        cat = 0\n",
    "        for i, v in enumerate(thresholds):\n",
    "            if prob > v:\n",
    "                cat = i + 1\n",
    "        return cat\n",
    "\n",
    "    #Build a cross‚Äëclassification matrix showing, for a subset of patients, how often they move\n",
    "    #from one risk category under the reference model to another category under the new model.\n",
    "    def make_cat_matrix(ref, new, indices, thresholds):\n",
    "        num_cats = len(thresholds) + 1\n",
    "        mat = np.zeros((num_cats, num_cats))\n",
    "        for i in indices:\n",
    "            row, col = check_cat(ref[i], thresholds), check_cat(new[i], thresholds)\n",
    "            mat[row, col] += 1\n",
    "        return mat\n",
    "\n",
    "    def calculate_nri_at_time(y_val, ref_probs, new_probs, time_point, thresholds):\n",
    "        # Identify individuals at risk at this time point\n",
    "        mask = y_val[\"RFS_time_Months\"] >= time_point\n",
    "        y_filtered = y_val[mask]\n",
    "        indices = y_filtered.index.tolist()\n",
    "    \n",
    "        if len(indices) == 0:\n",
    "            return {\"time\": time_point, \"nri_event\": np.nan, \"nri_nonevent\": np.nan, \"nri_total\": np.nan}\n",
    "    \n",
    "        y_truth = y_filtered[\"RFS_Status\"].values\n",
    "        ref = ref_probs[indices]\n",
    "        new = new_probs[indices]\n",
    "    \n",
    "        event_index = [i for i in range(len(y_truth)) if y_truth[i] == 1]\n",
    "        nonevent_index = [i for i in range(len(y_truth)) if y_truth[i] == 0]\n",
    "    \n",
    "        event_mat = make_cat_matrix(ref, new, event_index, thresholds)\n",
    "        nonevent_mat = make_cat_matrix(ref, new, nonevent_index, thresholds)\n",
    "    \n",
    "        # upward = more aggressive classification (toward high risk)\n",
    "        events_up = np.triu(event_mat, k=1).sum()\n",
    "        events_down = np.tril(event_mat, k=-1).sum()\n",
    "    \n",
    "        nonevents_up = np.triu(nonevent_mat, k=1).sum()\n",
    "        nonevents_down = np.tril(nonevent_mat, k=-1).sum()\n",
    "    \n",
    "        n_events = len(event_index)\n",
    "        n_nonevents = len(nonevent_index)\n",
    "    \n",
    "        nri_events = (events_up - events_down) / n_events if n_events > 0 else np.nan\n",
    "        nri_nonevents = (nonevents_down - nonevents_up) / n_nonevents if n_nonevents > 0 else np.nan\n",
    "    \n",
    "        return {\n",
    "            \"time\": time_point,\n",
    "            \"nri_event\": nri_events,\n",
    "            \"nri_nonevent\": nri_nonevents,\n",
    "            \"nri_total\": nri_events + nri_nonevents if not np.isnan(nri_events + nri_nonevents) else np.nan\n",
    "        }\n",
    "\n",
    "    def compute_nri_for_survival_models(model_ref, model_new, X_val_base, X_val, y_val, time_points, thresholds):\n",
    "        # ‚úÖ Reset index to align with NumPy arrays\n",
    "        y_val = y_val.reset_index(drop=True)\n",
    "        \n",
    "        # Predict survival functions\n",
    "        surv_funcs_ref = model_ref.predict_survival_function(X_val_base)\n",
    "        surv_funcs_new = model_new.predict_survival_function(X_val)\n",
    "    \n",
    "        # Convert to risk probabilities (1 - S(t)) at each time point\n",
    "        def get_risk_probs(surv_funcs, times):\n",
    "            return np.asarray([[1 - fn(t) for t in times] for fn in surv_funcs])  # shape (n_samples, len(times))\n",
    "    \n",
    "        risk_probs_ref = get_risk_probs(surv_funcs_ref, time_points)\n",
    "        risk_probs_new = get_risk_probs(surv_funcs_new, time_points)\n",
    "    \n",
    "        results = []\n",
    "        for i, t in enumerate(time_points):\n",
    "            result = calculate_nri_at_time(\n",
    "                y_val,\n",
    "                ref_probs=risk_probs_ref[:, i],\n",
    "                new_probs=risk_probs_new[:, i],\n",
    "                time_point=t,\n",
    "                thresholds=thresholds\n",
    "            )\n",
    "            results.append(result)\n",
    "    \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    X_val2 = train_df[features_2]\n",
    "    time_points = np.percentile(y_val['RFS_time_Months'], [25, 50, 75])\n",
    "    percentiles = [25, 50, 75]\n",
    "    # Define risk thresholds for category classification\n",
    "    risk_thresholds = [0.02, 0.1, 0.5, 0.95]\n",
    "    # Run NRI computation ---- fo rf model \n",
    "    nri_df = compute_nri_for_survival_models(model_rf, model_rf_final, X_val, X_val2, y_val, time_points, risk_thresholds)\n",
    "    nri_df[\"percentile\"] = percentiles\n",
    "    nri_df[\"random_state\"] = random_state\n",
    "    nri_df[\"model\"] = \"random_forest\"\n",
    "    nri_df[\"comaprison\"] = f\"\\nüîÅ Running comparison: {model1_name} vs {model2_name}\"\n",
    "    output_csv = \"./Output/NRI/NRI_train.csv\"\n",
    "\n",
    "    # Save or append to CSV\n",
    "    try:\n",
    "        existing = pd.read_csv(output_csv)\n",
    "        combined = pd.concat([existing, nri_df], ignore_index=True)\n",
    "        combined.to_csv(output_csv, index=False)\n",
    "    except FileNotFoundError:\n",
    "        nri_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    lasso_df = compute_nri_for_survival_models(model_lasso, model_lasso_final, X_val, X_val2, y_val, time_points, risk_thresholds)\n",
    "    lasso_df[\"percentile\"] = percentiles\n",
    "    lasso_df[\"random_state\"] = random_state\n",
    "    lasso_df[\"model\"] = \"LASSO\"\n",
    "    lasso_df[\"comaprison\"] = f\"\\nüîÅ Running comparison: {model1_name} vs {model2_name}\"\n",
    "    lasso_csv =  \"./Output/NRI/NRI_train.csv\"\n",
    "    # Save or append to CSV\n",
    "    try:\n",
    "        existing = pd.read_csv(lasso_csv)\n",
    "        combined = pd.concat([existing, lasso_df], ignore_index=True)\n",
    "        combined.to_csv(lasso_csv, index=False)\n",
    "    except FileNotFoundError:\n",
    "        lasso_df.to_csv(lasso_csv, index=False)\n",
    "\n",
    "    # Run NRI computation ---- fo rf model \n",
    "    elas_df = compute_nri_for_survival_models(model_elastic, model_elastic_final, X_val, X_val2, y_val, time_points, risk_thresholds)\n",
    "    elas_df[\"percentile\"] = percentiles\n",
    "    elas_df[\"random_state\"] = random_state\n",
    "    elas_df[\"model\"] = \"ELASTIC_NET\"\n",
    "    elas_df[\"comaprison\"] = f\"\\nüîÅ Running comparison: {model1_name} vs {model2_name}\"\n",
    "\n",
    "    # Set file to store cumulative NRI results\n",
    "    elas_csv = \"./Output/NRI/NRI_train.csv\"\n",
    "    # Save or append to CSV\n",
    "    try:\n",
    "        existing = pd.read_csv(elas_csv)\n",
    "        combined = pd.concat([existing, elas_df], ignore_index=True)\n",
    "        combined.to_csv(elas_csv, index=False)\n",
    "    except FileNotFoundError:\n",
    "        elas_df.to_csv(elas_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49981e-2d3b-46dd-bce3-95723d3ab41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
